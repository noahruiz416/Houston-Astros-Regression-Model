# Houston-Astros-Regression-Model
Complete Regression Model Analysis for predicting houston Astros wins, full report is below:

## Executive Summary
Every year from March to October, millions of Americans and citizens around the globe tune in to watch one of Americas most beloved games, baseball. The cultural impact of baseball among society is vast and has led to the game becoming an integral part of American society. Additionally, Americas passion for baseball has led to the sport of baseball becoming a substantial market in the world of sports business. According to reports by Forbes the average MLB franchise is valued at around 1.9 billion dollars with the average franchise brining in about 122.1 million dollars of revenue each year (Statista).
Naturally societies love for baseball and sports has created a whole industry regrading gambling and sports prediction. This has led to sports gambling becoming a substantial industry, in which many fans, use biased facts and opinions to try and â€˜predictâ€™ who will win a baseball game.
This poses the question; how we can model and understand some of the factors that lead to a baseball team winning a game? In this report I propose two complete first order multivariable regression models that will accurately model baseball wins. For this report data was selected from the Houston Astros and the two models were created with data ranging from 1979-2000 & 2001-2021 respectively.
After finishing the exploratory analysis 5 main variables were narrowed down through correlation analysis, with several initial models being created. The respective variables being Runs, Runs Batted In, On Base Percentage, On Base Slugging Percentage and Slugging Percentages. However, each of those models were weak and additional analysis was needed and several other hypotheses were tested. Finally, I concluded that the best fit model uses; â€˜Losesâ€™, â€˜Runs Scoredâ€™, â€˜Hits Allowedâ€™ and interaction between â€˜Loses and Hitsâ€™ to model wins for the earlier time frame (1979 - 2000). On the other hand, the best fit model for the recent time frame (2001 - 2021) uses Runs, RBIs, Errors Committed and Errors Committed squared as input variables. Additionally, each of the best fit models reported high adjusted R-squares (>0.90) and low p values for the T and F tests indicating good fit and statistical significance.
In this report, extensive exploratory analysis, description of methods, regression techniques and more follow, all with the purpose of describing our models.

## Data
The purpose of this study is to create two best â€˜fittingâ€™ models to predict wins for the Houston Astros. Our two respective model both were created with data from two various timeframes. For simplicityâ€™s sake we will refer to the model that was created with data from 1979 â€“ 2000 as â€œModel 1â€ and the model created with data from 2001 â€“ 2021 as â€œModel 2â€. The data itself was sourced from the â€œbaseball reference teamsâ€ site, though a link to the site will be included in the appendix below. The dataset also has a total of 60 observations when aggregated together, or when split into their respective groups â€œModel 1â€ was created with 20 observations and â€œModel 2â€ with 20. Additionally, the years 1994, 1995 and 2021 were not included from either Model,
 
because each of those years respective baseball season was cut short. This would in turn effect the accuracy of the either Model, so they are left out. On a final note, the dataset included a data dictionary that was used various times throughout this report to understand important variables.

## Methodology
Various statistical methods and techniques are used through this report to solve the proposed research question. In this section I will break down each of the main methods that were used within this study. Additionally, from this point on there will be two respective sections for the Exploratory Data Analysis and the Multivariate regression to factor in different findings from the years ranging 1979 â€“ 2000 and the years ranging from 2001 â€“ 2021. Without going into the details however each timeframe was first treated with correlation analysis. Correlation analysis was conducted on the dataset to check for any correlation between our given input variables and our predicted output variable, â€˜winsâ€™. Additionally due to the large number of variables given, correlation analysis allowed me to narrow down the scope of my problem. Next extensive EDA was conducted on the variables that showed either a strong negative or positive relationship with wins. EDA is necessary to understand underlying distributions, relationships and outliers lying within our dataset. Further, the EDA allowed me to visualize the correlation coefficients derived from the correlation analysis section. A plethora of graphs were used in the EDA such as boxplots, scatter plots, histograms, and effect plots. Finally with the variables selected two complete models were created. However, creating each respective model was quite the iterative process, and it took multiple attempts and reiterations of the process described above before the models of best fit where found. On a final note, I used some of my own domain knowledge regarding baseball to test various hypotheses regarding interaction between variables, though a more in-depth discussion regarding that will occur later in this report

## Correlation Analysis
For model 1 and model 2, due to the large number of possible input values to predict wins, correlation analysis was utilized to narrow the scope of the project and pave the way for exploratory data analysis and eventual model development. Using SAS, correlation coefficients were computed for each continuous variable. I focused on our dependent variable â€˜winsâ€™ and tried to find any variables which may have some relationship with the variable. In this case by â€˜someâ€™ I mean a moderate or strong relationship with â€˜Winsâ€™. In the case for both models 1 and model 2 we can define a moderate relationship as:
ğ‘Ÿâ‰¥.50 âˆªğ‘Ÿâ‰¤âˆ’.50
Where the â€˜râ€™ represents a correlation coefficient value that is either showing a positive or negative relationship with wins. Further we assume that â€˜râ€™ is computed for each of the variables with respect to wins. With the â€˜moderateâ€™ relationship defined I then ran correlation function and was able to find interesting results for that had useful implications for both model 1 and model 2.

## Model 1 Results Correlation Analysis:
Running the correlation analysis with â€˜Model 1â€™ data ranging from 1979 â€“ 2000, led to some interesting findings regarding potential relationships between input variables and wins. With the definition of a â€˜moderateâ€™ relationship above I search the output correlation values for anything that met the given criteria. The results were very pleasing and listed below is a table summing up what I found regarding model 1â€™s â€˜strongestâ€™ correlation values. Though I will save the actual analysis of these variables for later, right now we can see that there are several variables that have potential to be used in our final model. For now, though we can see that Losses and Finish both are moving in a negative linear direction with correlation coefficients less than -0.50. Moving to the positive coefficients we see that 2B, RBI and SB all move in a positive direction with a coefficient greater than 0.50.

<img width="406" alt="Screen Shot 2022-04-29 at 9 17 56 PM" src="https://user-images.githubusercontent.com/88412646/166090331-fa3e4a83-5968-452a-802a-08e1d59d737e.png">


## Model 2 Results Correlation Analysis:
After running model 1â€™s correlation matrix, similar treatment was done to model 2. My findings for model 2â€™s time range were very surprising. There is a significant difference between the variables that are correlated between model 1 and model 2. This can be due to multiple factors such as team management, rosters and more, though it is out of the scope for this project. Going back to the actual results they are summed up in a similar table below and like in the section above we will save the actual analysis of the variables for the section below. Losses and Finish increased greatly regarding negative correlations. Additionally, â€œRunsâ€ also appeared as a new indicator variable with a high positive correlation coefficient. 2B also was dropped as a potential model predicator as the correlation coefficient was relatively low compared to the other values that appear within model 2â€™s time range. The two other variables we narrowed down with model 2â€™s correlation analysis are RBI and R/G. These variables were added because they were very strong and show a high linear positive correlation to wins.

<img width="407" alt="Screen Shot 2022-04-29 at 9 18 10 PM" src="https://user-images.githubusercontent.com/88412646/166090453-caeb4f70-185f-4943-92be-d15273837416.png">

## Exploratory Data Analysis Results Model 1
 
After taking into consideration the results from Model 1â€™s correlation analysis, extensive exploratory data analysis was conducted on the dataset. As mentioned above particular emphasis was put on â€œLossesâ€, â€œFinishâ€, â€œ2Bâ€, â€œRBIâ€ and â€œSBâ€. Additionally, I conducted analysis on the actual â€œWinsâ€ column to gain a better understanding of the teamâ€™s overall performance in this timeframe. For the exploratory analysis, I decided that there were three main graphs that would allow me to better understand the spread, shape and distribution of possible model inputs and the dependent variable as well. First, I normalized the data frame, by applying the standardization formula.

I chose to standardize the dataset to better interpret the actual shape and distribution of each of the graphs created and to also avoid any errors when analyzing graphs with different unit scales. The first batch of graphs created for the first model were a group of probability distributions. Losses tend to follow a slightly left skewed distribution, this indicates that in the older time the Houston Astros must have been lacking in terms of performance as few years exist in which the number of losses is low. Instead, the data is skewed to the left implying that the team has bad performance in the past. Next, I analyzed the shape of the Finish variable, which measures the teams finish for career totals. The finish variable was slightly skewed to the right and did not necessarily follow a normal distribution. Moving onto the doubles hit and allowed, the data follows a normal distribution with a slight skew to the right indicating that the number of doubles hit / allowed stays relatively constant but tends to be slightly higher due to the slight skew to the right. RBIâ€™s follow a normal distribution as well, with a slight skew to the right indicating that for Model 1, RBIs tend to fall around a similar mean but can also be quite high depending on the season. Stolen bases, seem to have no shape at all regrading normality, and are slightly skewed to the left. This can be interpreted as very volatile and reflects the high risk of stealing bases in a game of baseball. Finally looking at the shape of wins there tends to be a slight skew to the right, however the left tail is quite heavy as well, indicating that from this timeframe the team had subpar performance. However, it is hard to pin down specific factors due to the large time frame that we are analyzing and the arbitrary nature of the time frame itself.
After checking the shape of the variables, I next analyzed boxplots for each variable. Boxplot analysis allowed me to determine any interesting outliers within the dataset, giving more insight into the Astros performance from 1979 â€“ 2001. In the lossâ€™s variable, there is a very low, lower limit, indicating that the team performed very well during some years. Additionally there is one datapoint that even lies outside of the lower limit range, indicating an exceptional year in which the Astros lost very few games. However, we can also say something negative about the teamâ€™s performance, as most of the datapoints lie slightly to the right of the median. This indicating that on average the Astros performance was subpar. Moving onto the â€˜Finishâ€™ variable two outliers lie outside the boxplot however everything still tends to be centered around the median. Doubles and Hits allowed, follows the same even distribution as mentioned before, with one outlier on the right limit, indicating a year in which a high number of doubles was scored. RBI also has several outliers on beyond the upper and the lower limit of the box plot, however most data points are clustered around the median. Stolen bases, has no outliers at all, and the left skewed nature of the variable shows on the boxplot as well, with a lower limit 2 standard deviations away from the median. Finally wins shows a very similar relationship to losses, with one exceptional year
where wins were above the upper limit. However subpar performance is still implied due to the high number of observations lying within -2 to -1 standard deviations from the mean.

Next, I created scatter plots for each of the 5 variables I narrowed down my search too and found that all the correlation coefficients, found in the correlation analysis, follow a similar direction that I predicted using the correlation coefficients. RBIâ€™s, Wins and Doubles / Hits allowed, all have moderate positive linear association with Wins, confirming that they may help predict wins. Interestingly Stolen Bases, seems to have very little association with wins when graphed on a scatter plot, because of this I decided to drop this variable as a potential model input value. The Finish and Losses variables both showed strong negative linear association with wins, indicating that they may have great potential for helping predict wins. Finally, I conducted Ad Hoc analysis on the â€˜Hitsâ€™ variable which I decided to do after dropping the Stolen Bases variable. I created an effect plot between Stolen Bases and Losses to test for any interaction between the two variable and found that they are both highly dependent on one another. I hypothesized this relationship, because of the rules of baseball. Losses occur when you lose to another team from points scored by passing home plate. If a team gives up a high number of hits, it will imply that their chances of winning the game go down substantially as more batters are given the opportunity to pass home plate. After conducting the EDA for model 1, that left me with the variables Losses, Finish, Doubles Scored / Allowed, RBI, and hits allowed, for possible predictors of wins.

## Exploratory Data Analysis Results Model 2:
For Model 2 similar methodology was used to determine what variables to further explore. The results of the correlation analysis narrowed our indicator variables to Losses, Teams Finish, Rungs Scored, RBI and Runs Scored Per Game. The wins column was also analyzed to gain further insight into how the team performed from 2001 â€“ 2021. The three main graphs used to analyze these five variables were probability distributions, boxplots, and scatter plots. All three of these graphs enabled me to analyze each of these variables in depth and narrow down to three input variables. For the timeframe in 2001 â€“ 2021, losses were skewed slightly to the right but followed a relatively normal distribution. In terms of team performance this indicates that losses tend to be lower from 2001 â€“ 2021 compared to the time frame 1979 â€“ 2000 when we just look at the graphs themselves. Interestingly Teams Finished, Runs Scored, RBI and Runs Scored Per Game do not follow any set distribution and large amounts of curvature and variation exists within each respective probability distribution. Additionally, the wins variable seems to be slightly skewed to the left, indicating that there were few years in which losses were at a minimum. This indicates that from 2001 â€“ 2021, the Astros performance increased as more of the wins are in the right tail of the probability distribution. Later in the report we will conduct a T-Test on average wins and losses for each respective time blocks to see if there is a statistically significant difference in team performance between our two respective timeframes.
After analyzing the probability timeframes form Model 2, I then created boxplots for each of the variables. The losses boxplot shows that most losses are centered around the median showing low amounts of variation near the median. However, there is also a very high upper limit within the lossâ€™s boxplot, showing that some years do have a higher number of losses compared to the median. The teams finish variable seems to have a high number of observations above the median, showing very little normality in the Teams Finish variable. Runs Scored, RBI and Runs
 
Scored Per Game all show very similar characteristics, with very high amounts of variation and distance between each of the quartile ranges. Finally, the wins variable also has a large lower limit indicating the skew of the data to the left and that during one of the seasons from 2001 â€“ 2021 wins were at a low and team performance was very poor. Analyzing the boxplots shows the high amounts of variation within the majority of Model 2â€™s variables while also showing that performance tended to be better from 2001 â€“ 2021.
Next, I plotted each of the indicator variables on scatterplots against wins. When plotting losses against wins a very strong negative linear relationship is shown between losses and wins. In context of baseball, this makes perfect sense as if a team has a higher number of losses, it indicates that they did not win the game. Teams finish also followed a negative linear relationship, showing that it may make a good indicator for predicting wins. Runs Scored was plotted against wins next and a strong linear relationship is shown between the number of runs the Astros scored and the wins. RBI and Runs Scored Per game also have a strong linear relationship with respect to wins. Finally, I wanted to test for interaction between losses and hits allowed as I did in my past model and found that there is interaction between hits and losses even in model 2.

## Wins and Loses T-Test:
While conducting the exploratory data analysis it seemed that there was a difference in average losses and average wins, from the time periods 1979 â€“ 2000 and 2001 â€“ 2021. To confirm this hypothesis, I decided to set run a t-test to see if there is a statistically significant difference between win means and loss means, in the two different time frames. In both t-Tests I ran I also assumed independence year after year. First, I tested the difference between win means. We can write a hypothesis test for the difference in means as follows (alpha = 0.05):
ğ»0: ğœ‡ğ‘œğ‘™ğ‘‘ = ğœ‡ğ‘›ğ‘’ğ‘¤
ğ»ğ‘:ğœ‡ğ‘œğ‘™ğ‘‘ â‰ ğœ‡ğ‘›ğ‘’ğ‘¤

From the results there is not much of a significant difference between Wins from the older and more recent time frame. The actual variance seems to be much greater in older years, however the actual Means themselves are very similar. Additionally, the corresponding p value is greater than our significance level 0.05. This means we do not have enough evidence to reject the null hypothesis, which indicates that there is not a statistically significant difference in mean wins between the time periods 1979 â€“ 2000 and 2001 â€“ 2021.
Additionally, I ran a similar t-Test for mean loses, to try and see if a statistically significant difference exists between each respective period. For the t-test we can write the hypothesis test as follows and assume alpha = 0.05:
ğ»0: ğœ‡ğ‘œğ‘™ğ‘‘ = ğœ‡ğ‘›ğ‘’ğ‘¤
ğ»ğ‘:ğœ‡ğ‘œğ‘™ğ‘‘ â‰ ğœ‡ğ‘›ğ‘’ğ‘¤

Going over the results there is immediately very little difference between each respective mean value. Though the variance is much less in recent years, indicating predictable and consistent performance, the results do not show much significance because our two tailed p value is > 0.05. Because of this we can say that we do not have enough evidence to reject the null hypothesis and that there is not a statistically significant difference between mean loses. Despite neither of our t- tests showing significance, we gained insight into the higher amounts of variation that occurred in both wins and losses from 1979 â€“ 2000. This implies that from 1979 â€“ 2000, the Houston Astros had less consistent performance as more variation occurred around the mean wins and losses.

## Multivariate Regression Analysis Model 1:
With the information from the EDA, we can now build a model to try and predict the Houston Astros wins from 1979 â€“ 2000. To create a model that will predict wins, we must first set up a
  
hypothesis test around multiple parameterâ€™s ğ›½. Assuming a significance level were ğ›¼ = 0.05 we an write a hypothesis test around 3 parameters as follows.
ğ»:ğ›½+ğ›½+ğ›½=0 0123
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0
After hypothesizing around our parameters, we can now create several multiple regression models based on the information we gained in the exploratory analysis and test each individual model parameter for statistical significance and model fit. As mentioned in the EDA for model 1, we will use the variables Losses, Finish, Doubles Scored / Allowed, RBI, and hits allowed during the model creation process. For experimentation, the first thing I did was create a complete model that used all five of our best â€˜fitâ€™ variables that were discovered during the correlation analysis. Though our final model will not incorporate all five variables, I thought it would be best to first understand what it would look like if we created a model incorporating all five input parameters. To test for all five parameters, we must first set up a hypothesis test for 5 input parameters. The test will be very similar to the one above for three parameters with a few differences regarding the number of inputs. We write the test as follows (assume alpha = 0.05):
ğ»:ğ›½+...+ğ›½ =0 015
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0
After setting the hypothesis test, we run the model and with our new coefficients we can write the regression equation as follows:
ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘Šğ‘–ğ‘›ğ‘  (ğ‘¦) = 21.2854 + 0.065 (ğ·ğ‘œğ‘¢ğ‘ğ‘™ğ‘’ğ‘ ) + 0.093 (ğ»ğ‘–ğ‘¡ğ‘ ) âˆ’ 0.5605 (ğ¿ğ‘œğ‘ ğ‘ ğ‘’ğ‘ ) âˆ’ 0.8004 (ğ¹ğ‘–ğ‘›ğ‘–ğ‘ h) âˆ’ 0.0629 ğ‘…ğµğ¼
Interpreting our model, we can say that we assume that the intercept starts at around 21 wins and for each additional double scored or allowed the wins increase by 0.065 and for each additional hit wins increase by 0.093. As a team loses increase the predicted wins decrease by 0.5605 as finish increases predicted wins decrease by 0.8004 and as RBIâ€™s increase the predicted wins decrease by -0.0629 points. Almost all the parameter estimates pass our stated alpha level and additionally in the models ANVOA table we have a high F statistic of 14.31 which subsequently reports a very low p value that is < .0001. From the F-Statistic alone we can say that our model is statistically significant since 0.0001 < 0.05, showing that at least one of our model parameters has a significant relationship for describing and predicting wins. However, as I mentioned not all the parameter estimates pass the T-test in the Parameter Estimates table. In this case Finish and RBI and Doubles all report P values > 0.05. Doubles reports a p value of 0.0651, Finish reports a p value of 0.5571 and RBI reports a p value of 0.018. Since all these values are > 0.05, we can take them out of future model iterations for â€œModel 1â€ and conclude that they are not statistically significant. Still though the model still shines some light on possible values that are statistically significant including Losses and Hits, where interaction exists as shown by our effect plots in the EDA. Additionally, further analyzing the model we report an adjusted R â€“ Square of 0.7779, meaning that around 79.79% of the variation in our wins can be explained by RBIâ€™s, Hits, Losses, Finish and Doubles Scored / Allowed. As stated previously this model, is more of an

attempt to screen variables before making our final 3 parameter model. In doing so we found that certain variables should be nulled from our final model and shed some importance on Losses and Hits.
With the results from the iteration number one of â€œModel 1â€, I decided to run a model with Runs, Hits and Losses. I decided to take out all the other variables we used in our five-parameter model that were not significant and instead used runs. The main reason I chose to use runs is because it also reported a relatively high correlation coefficient between wins. Additionally, from my own knowledge of baseball I hypothesized that the amount of Runs a team has, will most likely describe some relationship between wins and our regression model. For this model we can state the hypothesis test for the parameters as follows (assume alpha = 0.05):
ğ»:ğ›½+ğ›½+ğ›½=0 0123
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0
After stating the hypothesis test, I created the model in SAS and created the following regression equation with the given coefficients. We can write our three-parameter equation as follows:
ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘Šğ‘–ğ‘›ğ‘  (ğ‘¦) = 20.0633 + 0.1048 (ğ»ğ‘–ğ‘¡ğ‘ ) âˆ’ 0.0470 (ğ‘…ğ‘¢ğ‘›ğ‘ ) âˆ’ 0.6708 (ğ¿ğ‘œğ‘ ğ‘’ğ‘ )

Interpreting our model, we can say that when there are zero hits, zero runs and zero losses we will start at 20 initial wins. In terms of the problem itself, this does not necessarily make sense, however it is the intercept for our given model. Going into our interpretation of the model parameters, for each additional hits the predicted wins increase by 0.1048 points, for each additional run predicted wins decreases by 0.0470 and for each additional loss, predicted wins decreases by 0.6708. Next, we can check the ANOVA table and specifically the F-Test and subsequent p values to test for model utility. For this model we end up getting an F value of 23.59 and a p value < 0.001. In terms of statistical significance this indicates that our overall model is useful for modeling predicted wins, implying that at least one of the input parameters I statistically significant as well. After checking the overall model utility, we can also check the corresponding Parameter estimate t-tests, for statistical significance. Runs reports a p value of 0.0217, Hits reports a p value of <0.0001 and loses reports a p value < 0.0001. Assuming an alpha level of 0.05, that means that Runs, Hits and Loses all pass their respective t-tests since the p values are less than 0.05. This means that each of the input parameters are statistically significant and are effective as input parameters for modeling wins. After checking the overall model utility and the input parameters, I went over root MSE, Adj â€“ R -Square and the coefficient of variation to further analyze model performance. We report an adjusted R square of 0.7811, meaning that 78.11% of the variation in wins can be explained by Runs, Hits and Losses. Regarding model performance, this result is a good sign and indicates that a high amount of the variation is explained by our model. However there still is around 22% of variation in wins that is left unexplained by our model, which means improvement can possibly be made. Moving over to the Root MSE, we see that a relatively low value of 4.7635 is reported. In terms of the problem, this is a very good sign and indicates that our model is only off by about 4 wins on average. The coefficient of variation at 5.77% also shows the trend of low variation in our predicted wins, indicating good model fit and performance. Overall, this three-term model isufficient for predicting wins. The model is statistically significant in all input parameters (Runs, Hits/ Hits Allowed, Loses) and passes the test for overall model utility. Additionally, the majority of variance in predicted wins is explained by our model, further indicating good fit. However due to the adjusted R- square it led me to testing one more model iteration, this time with an interaction term, to try and push model performance further and increase the adjusted R- Square.

In the final iteration of â€œModel 1â€, I added an interaction term between Hits and Losses, because of the interaction I observed in the EDA analysis of model one. As I mentioned previously, I came up with the idea from my own former knowledge of baseball. Typically, the hits a team allows the more likely they are to lose, because it is the main scoring mechanism in baseball. Additionally, hits (not hits allowed) can lead to a team increasing their chances of winning due to the same reasons stated above. After plotting loses and hits on the effect plot with respect to wins it become clear that there is interaction between hits and loses. Because of this we can run a hypothesis test, to test our new parameter estimates. We write the test as follows:
ğ»:ğ›½+ğ›½+ğ›½+ğ›½ğ‘¥ğ‘¥=0 0123423
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0

After running the model in SAS, we can form a regression equation for our new interaction model as follows:
ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘Šğ‘–ğ‘›ğ‘  (ğ‘¦) = âˆ’160.1694 âˆ’ 0.010(ğ‘…ğ‘¢ğ‘›ğ‘ ) + 2.6846(ğ¿ğ‘œğ‘ ğ‘ ğ‘’ğ‘ ) + 0.2228(ğ»ğ‘–ğ‘¡ğ‘ ) âˆ’ 0.0025(ğ¿ğ‘œğ‘ ğ‘ ğ‘’ğ‘  âˆ— ğ»ğ‘–ğ‘¡ğ‘ )
Due to the nature of interaction terms, interpreting our new model is harder compared to previous iterations. The slope in this problem makes no sense and when Runs, Losses and Hits are set to zero it will leave us with a negative predicted wins value of -160. For each increase in runs predicted wins in turn decreases by -0.010 wins, for each increase in losses predicted wins increase by 2.6846 for each additional hit ran / allowed predicted wins increases by 0.2228 and for each additional loss and hit the interaction between those two terms causes predicted wins to decrease by 0.0025 wins. Though interpretability lacks in our new model, statistical significance does exist, as our ANOVA table reports an F value of 60.74 and a corresponding p value < 0.0001. This means that our significance level of 0.05 we can reject the null hypothesis and claim at least one of the parameters is statistically significant. Additionally, when we move to the parameter estimates table it becomes clear that our parameters are statistically significant as well. Loses reports a p value of 0.0004 and Hits reports a p value < 0.0001, the interaction between losses and hits also reports a p value < 0.001. The only variable that was not statistically significant was runs which reported a p value 0f 0.4362. Still though, since three of our model parameters pass the t-tests with p values < 0.05, we can reject the null hypothesis and state that there is a statistically significant relationship between our model parameters; Losses, Hits and interaction term and wins for the Houston Astros. Due to our statistically significant results, we can further analyze the regression output and ANVOA table. Our model reports an adjusted R square of 0.9263. This indicates that 92.63% of the variation in predicted wins can be explained by Runs, Loses, Hits and the interaction between loses and hits. When compared to the second iteration of â€œModel 1â€, this shows a huge improvement in model accuracy and performance leaving only 8% of the variation in predicted wins unexplained by our parameters. Next, we can

check the root MSE and see that on average only a 2.7671 deviation from actual wins and predicted wins. This is also a substantial improvement when compared to the previous iteration and indicates very good model fit. Finally checking the coefficient of variation, we have a value of 3.3467 percent which indicates a very low level of dispersion around the predicted and actual values. This further pushes the case for this iteration of our model being the best fit for predicting wins.
Overall, from the in-depth analysis on three different iterations on â€œModel 1â€ from the time 1979 â€“ 2000 for the Houston Astros, the third and final iteration of the model seems to be the model of best â€˜fitâ€™ because of the high adjusted R square, statistically significant results, low variation in predicted and actual values and low amounts of variance around the mean. In the next section I will use similar techniques to create a model of best fit for â€œModel 2â€.

## Multivariate Regression Analysis Model 2:

To find the model of best fit to predict wins from 2001 â€“ 2021, we will use a similar iterative process to the one described above to create the best fit multivariable regression model. Like the example above the first thing that must be done, is to create a five-parameter model with the input variables that we determined have the most substantial correlation coefficient in the correlation analysis section. In our case the five-order model can be written as follows:
ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘Šğ‘–ğ‘›ğ‘  (ğ‘¦) = 159.505 + 0.12038 (ğ‘…ğ‘¢ğ‘›ğ‘ ) âˆ’ 0.9889(ğ¿ğ‘œğ‘ ğ‘’ğ‘ ) + 0.01952(ğ¹ğ‘–ğ‘›ğ‘–ğ‘ h) âˆ’ 18.8680(ğ‘…ğ‘¢ğ‘›ğ‘  ğ‘ğ‘’ğ‘Ÿ ğ‘”ğ‘ğ‘šğ‘’) âˆ’ 0.00193(ğ‘…ğµğ¼)
Additionally for this test we can test the utility of the model parameters as follows:
ğ»:ğ›½+...+ğ›½ =0 015
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0
For the model checking the ANOVA table reports a p value < 0.0001, this shows that the overall model is statistically significant when compared at an alpha level of 0.05. For the variable runs we report a p value of 0.0008, for Losses the p value is < 0.0001, for finish the p value is 0.7909, for R/G the p value is 0.0011, and for RBI the p value is 0.7568. From this we can reject the null hypothesis since Runs, Losses and Runs Per Game all have p values < 0.05. Additionally, this allows us to say that this model is statistically significant. We can also analyze the adjusted R squared and see that 99.99% of the variation in wins is explained by Runs, Losses, Finish, Runs per game and RBIâ€™s. However, seeing the bizarre coefficients and overall uninterpretable nature of the model above I decided it would be best to reevaluate possible input variables, despite the results from the correlation analysis and the regression results in general indicating these given variables would result in a model of best fit.
Because of my decision here it led to me conducting a revaluation of possible input variables. I first decided to use the same parameters as the model above but to leave out the finish and loses variables. Since those two variables are extremely hard to interpret and ruin the significance of other input parameters, it caused me to try and create a model of best fit without the use of loses or finish. The reduced version of the original model uses the exact same input parameters except for loses and finish. For the model we can state the hypothesis test as follows:
 
ğ»:ğ›½+ğ›½+ğ›½=0 0123
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0
And given the coefficients from SAS output we can write the regression equation as follows:
ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘Šğ‘–ğ‘›ğ‘ (ğ‘¦) = âˆ’10.8863 + 1.312(ğ‘…ğ‘¢ğ‘›ğ‘ ) âˆ’ 229.7911(ğ‘…ğ‘¢ğ‘›ğ‘  ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘‘ ğ‘ƒğ‘’ğ‘Ÿ ğºğ‘ğ‘šğ‘’) + 0.2444(ğ‘…ğµğ¼)
Interpreting the model, we can see that the y intercept is set at -10.88 wins, meaning when Runs, Runs Scored Per Game and RBI are set to 0 the predicted wins is -10.88. Additionally for each additional runs predicted wins increases by 1.312, for each run scored per game predicted wins decreases by -229.7911 and for each increase in RBI predicted wins increases by 0.2444. To check model utility, we first check the ANOVA table for overall model utility. Since the F value is high it leads to a p value < 0.0001. Since the F value is high and the p value is < 0.05 we can say that the overall model is statistically significant. However, when we check the actual parameter estimates, a different story is shown. In the appendix below all the coefficients are reported. For this iteration of â€œModel 2â€, every single coefficient p value was > 0.05. This means we cannot reject the null hypothesis, allowing us to state that there is not a statistically significant relationship between our model parameters Runs, Runs Scored in game and RBI. Due to this, I stopped analyzing this model and concluded that it is insufficient for predicting wins. After failing to show any statistical significance with this specific group of variables, I decided to cycle back to the EDA and graphed various probability distributions, scatterplots and analyzed correlation coefficients for any new possible variables to use as input parameters. This led to a long process in which I iterated through around 4 different models before finally getting a model of best fit.
For timeâ€™s sake I will try and keep my analysis of the 4 models I iterated through brief. Regression equations for each subsequent model can be found in the appendix below. Also, for simplicities sake I will refer each subsequent model as â€œiteration 3...6â€ with the final eight model being the model of â€˜best fitâ€™. For iteration 3 I decided to use Errors Committed, Hits/Hits Allowed and RBI as the model input parameters to predict wins. For iteration 3 we can write the hypothesis test for our model as follows (assume alpha = 0.05):
ğ»:ğ›½+ğ›½+ğ›½=0 0123
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0

After creating the model in SAS, we can analyze the corresponding ANOVA table and Parameter estimates table to determine whether our model is statistically significant or not. To sum up the results the model pasted our test of overall model utility as the ANVOA table reported a p value < 0.0001. Next when analyzing the parameter estimates, both Errors Committed and RBI show statistical significance with p values < 0.05. Hits/Hits allowed reports a p value > 0.05, showing that Hits is not statistically significant. Overall, since more than one of our input parameters are statistically significant, we can reject the null and claim that iteration 3s model parameters show some relationship with wins. Additionally, iteration 3 shows relatively good fit. When analyzing the ANOVA table an adjusted R square of 0.8492, is shown. This

means that 84.92% of the variation in predicted wins is explainable by Errors Committed, Hits and RBIâ€™s. Additionally, a low root MSE of 6.0566 and a coefficient of variation of 7.36% push this argument as both mean that the actual variation between predicted and actual wins is relatively low. Though this model shows good fit, I decided that model performance could we improved, since around 16% of the variation is unexplained by the model.
In iteration 4, I decided to test for an interaction between Errors Committed and Hits / Hits Allowed. I tested for interaction because in terms of baseball it would make sense that a natural interaction exists between Errors committed and hits allowed. To do so I first graphed an effect plot, which ended up showing a clear relationship between the two variables. Additionally, I tested for interaction between Runs and Errors on an effect plot, showing a relationship between the variables as well. Going back to iteration 4 the parameters used to create the model are Runs Scored in the Game, Errors Committed, Hits/Hits Allowed and the interaction between Errors and Hits. These parameters were selected based off the high correlation coefficients that each parameter showed in the correlation analysis section. Like the iteration above the regression equation for this iteration will be listed in the appendix below. For this iteration we can write the hypothesis test around the model parameters as follows:
ğ»:ğ›½+ğ›½+ğ›½+ğ›½ğ‘¥ğ‘¥=0 0123412
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0

From the regression output we can immediately check the ANVOA table for overall model fit. At an alpha level of 0.05, we can conclude that overall model utility is statistically significant since the ANOVAâ€™s p value is < 0.05. Further analyzing the parameter estimates will show that every single one of the coefficients is significant. RG reports a p value < 0.0001, Errors reports a p value of 0.0373, hits report a p value of 0.045 and the interaction between hits and errors is 0.056. Additionally, the model shows greater fit with an adjusted R-square of 0.8834. This indicates that around 88.34% of the variation in wins can be explained by the Runs Scored Per Game, Errors Committed, Hits / Hits Allowed and the interaction between Errors and Hits. Further the root MSE for the model is 5.3249 and the coefficient of variation lies at 6.47%. Each of these regression statistics indicates a high amount of explainable variation, low error between predicted and actual values and low variation around the mean.
In iteration 5, I decided to create a similar interaction model to iteration 5 however I created interaction between Errors Committed and Runs / Runs Allowed. I hypothesized the interaction from my own knowledge of baseball, as it would be natural for a relationship to exist between an error and a run being allowed. I then graphed an effect plot that between Errors Committed and Runs allowed / Runs, which showed a clear relationship between the two variables. We can then create our model in SAS and test for significance with the following hypothesis tests:
ğ»:ğ›½+ğ›½+ğ›½+ğ›½ğ‘¥ğ‘¥=0 0123412
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0

Assuming an alpha level of 0.05, we can first check the ANOVA table for overall model significance. Our F value reports a p value < 0.0001, with our given alpha level we can claim that there is overall model utility. After checking model utility, we can test each individual parameter estimate. Runs Allowed reports a p value of 0.059, Runs per game reports a p value of 0.0296, and the interaction between Runs and Error reports a p value of 0.1061. Overall, the parameter estimates at our given model do pass our hypothesis test, as Errors committed has a p value < 0.05. However, the model still has poor parameter significance as the model parameters, Runs, Runs Scored Per Game and the interaction between Runs and Errors are not statistically significant at an alpha of 0.05. This caused me to disregard this model as a potential best fit model, due to the overall lack of significance, in most of the parameter values. Though the model does have good fit regarding the adjusted R square, with 90.13% of the variation in wins being predicted by Runs, Runs Per Game, Errors Committed and the interaction Between Runs and Errors Committed, the low parameter significance causes me to disregard the model.
Moving on to iteration 6, I will put more analysis into model 6 and will include both the regression equation and subsequent interpretation because this is the final iteration I went through. In the 6th iteration I decided to plot Errors committed on a scatterplot with Wins on the y-axis. What I found was that slight curvature exists when plotting errors. Because of this I decided to create a second order term, by squaring Errors committed. Additionally, the parameters Runs, RBI and Errors Committed were used in the model itself, alongside our newly created Errors Committed Squared term. Below we can write the hypothesis test for our second order model as follows:
ğ»:ğ›½+ğ›½+ğ›½+ğ›½ğ‘¥2 =0 012343
ğ»ğ‘: ğ´ğ‘¡ ğ¿ğ‘’ğ‘ğ‘ ğ‘¡ ğ‘œğ‘›ğ‘’ ğ›½ğ‘– â‰  0
With the hypothesis test created we can derive the regression coefficients with SAS and write the regression equation as follows:
ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ ğ‘Šğ‘–ğ‘›ğ‘ (ğ‘¦) = âˆ’28.8355 + 0.5734(ğ‘…ğ‘¢ğ‘›ğ‘ ) âˆ’ 0.4798(ğ‘…ğµğ¼) + 1.0377(ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘ ) âˆ’ 0.0079(ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘  âˆ— ğ¸ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿğ‘ )
When Runs, Errors and RBI are equal to zero the predicted wins are -28.8355. In terms of the problem itself this makes no sense at all and only works when input values are fed into the model. For each additional increase in runs predicted wins increase by 0.5734, for each RBIâ€™s increase predicted wins decrease by 0.4798, for each increase in errors predicted wins increase by 1.0377 and for each error we can model the curvature as the error squared multiplied by 0.0079, indicating a decrease in predicted wins. After interpreting the model, we can check the ANOVA table for overall model utility. With a p value < 0.0001, we can see that the overall utility of the model is significant assuming an alpha level of 0.05. However, we must go into the parameter estimates table to test for each individual parameterâ€™s statistical significance. The parameter Runs/Runs Allowed reports a p value of 0.014, RBIâ€™s reports a p value of 0.038, Errors committed reports a p value of 0.1381, and Errors Squared reports a p value of 0.0425. Overall, since each of the model parameters, except for Errors Committed, is < 0.05, we can conclude that each of the model parameters is statistically significant. Additionally, since we created a higher order term, the p value of Errors committed can be ignored. Thus, we can reject the null hypothesis and claim that there is a statistically significant relationship between each of

our model parameters and predicted wins. Testing for model fit we report an adjusted R square of 0.9086. This indicates that 90.86% of the variation in wins can be explained by Runs, RBIs, Errors Committed and Errors Committed Squared. Additionally, the ANVOA table reports a root MSE of 4.7156 which is relatively good in comparison to previous iterations. Finally, there is a coefficient of variation of 5.737% indicating relatively stable amounts of variation around the mean. Because of the statistical significance in the model parameters, interpretable regression equation, high adjusted R square, relatively low root MSE and low coefficient of variation, this led to me concluding that iteration 6 is the best fit model for predicting wins for the Houston Astros from 2001 â€“ 2021.

## Regression Results Summary and Comparison
Overall using various combinations of parameter inputs, we were able to create two â€˜best fitâ€™ models for Houston Astro wins from 1979 â€“ 2000 and 2001 â€“ 2021. From a pure perspective of model accuracy, the first model created for the period between 1979 â€“ 2000, is a better fit model. It reports a higher adjusted R square and includes statistically significant model parameters and significant interaction between hits and loses. However, the only downside with the first model is that the model does not really describe any interesting relationships beside the interaction between losses and hits/hits allowed. It is clear even without statistical modeling that wins can be predicted by losses. Although the first model is more accurate it has uninterpretable results and does not really help deepen our understanding of the factors that have a relationship with the Houston Astros winning a game. So instead, I propose that the second model created with data from 2001 â€“ 2021, is a better fit model because it is more interpretable and reveals interesting relationships between the model parameters (Runs/Runs allowed, RBIâ€™s, Errors committed, Errors committed Squared) and Houston Astros wins. Additionally, the second model also has good fit with a high adjusted R square (>0.90), low MSE, low coefficient of variation and statistically significant results.

## Conclusions and Next Steps
Like any other sport, baseball is an extremely complex game to try and model. The large number of variables in play during a game are seemingly unpredictable and random in nature. This causes many sports hobbyists to make seemingly inaccurate predictions regarding a teams win rate and in turn, cause them to lose thousands of dollars in sports betting and booking.
Through in-depth analysis of multiple regression models and an iterative model testing an analysis approach, I was able to nail down two regression models for predicting the wins of the Houston Astros from the time periods of 1979 â€“ 2000 and 2001 â€“ 2021. The first model from 1979 â€“ 2000 takes in Runs, Losses, Hits and interaction between Hits and Losses, as input parameters. The model itself is fit quite well with high adjusted R square values and statistically significant results. The second model for 2000 â€“ 2021, uses Runs, RBI, Errors and Errors Squared as the input parameters for predicting wins. Model two also reports a high adjusted R square, low coefficient of variation and statistically significant results. Additionally interpreting the model is also much easier compared to different iterations in the same timeframe.
  
Even though I was able to develop two models that fit very well to the given dataset there are a few qualms that I have with both models. First, I believe that raw game data would be useful in producing a better model. Raw data would allow me to fine tune the model itself and would give many more fields, potentially increasing the accuracy of the model. Additionally, I am concerned that many factors in baseball regrading actual location, managers, and players were not included in the given dataset. Having data on game location, managing team and actual player lineups could possibly allow for more in-depth analysis and results. Taking steps to collect more data could potentially lead to better insights regrading predicting team wins. Finally, having access to more domain knowledge would be useful as well. For example, researching more into baseball and interviewing avid fans or players could lead to useful insights, which I could have tried to model statistically.

## Appendix:

<img width="553" alt="Screen Shot 2022-04-29 at 9 21 40 PM" src="https://user-images.githubusercontent.com/88412646/166090724-10df62ee-895a-4e43-9c87-87e48c446f74.png">

<img width="574" alt="Screen Shot 2022-04-29 at 9 22 02 PM" src="https://user-images.githubusercontent.com/88412646/166090740-e2390fac-110e-4bb5-9665-40cf9555739a.png">

<img width="544" alt="Screen Shot 2022-04-29 at 9 22 11 PM" src="https://user-images.githubusercontent.com/88412646/166090747-e6a6796b-2975-46af-8605-017d4536856b.png">

<img width="565" alt="Screen Shot 2022-04-29 at 9 22 21 PM" src="https://user-images.githubusercontent.com/88412646/166090754-96d3ccc7-edcf-4531-8f4b-f1ec0a403107.png">

<img width="479" alt="Screen Shot 2022-04-29 at 9 22 47 PM" src="https://user-images.githubusercontent.com/88412646/166090765-a970bddb-0a76-4b28-86b2-acce924874a4.png">

<img width="486" alt="Screen Shot 2022-04-29 at 9 23 08 PM" src="https://user-images.githubusercontent.com/88412646/166090778-fdb8740c-b679-4509-86cd-fe205b469f0c.png">


